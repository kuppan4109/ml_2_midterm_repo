{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ML II – Midterm Project\n",
    "# Predicting Gym Member Experience Level Using a Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Definition\n",
    "\n",
    "**Business Context:**  \n",
    "A fitness centre wants to automatically classify a new member's experience level (Beginner, Intermediate, or Advanced) based on observable workout metrics collected during their first sessions. Knowing a member's experience level early allows the gym to:\n",
    "\n",
    "- Assign the right trainer and programme, reducing injury risk  \n",
    "- Personalise nutrition and hydration recommendations  \n",
    "- Improve member retention by avoiding over- or under-challenging new members  \n",
    "\n",
    "**Objective:**  \n",
    "Build a neural network that predict a gym member experince level - **Experience_Level** (1 = Beginner, 2 = Intermediate, 3 = Advanced) from biometric and workout data.\n",
    "\n",
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # keep output clean\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('gym_members_exercise_tracking.csv')\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and look for missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7h8i9j0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8i9j0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of our target variable\n",
    "print(\"Experience Level distribution:\")\n",
    "print(df['Experience_Level'].value_counts().sort_index())\n",
    "print(\"\\nWorkout Type distribution:\")\n",
    "print(df['Workout_Type'].value_counts())\n",
    "print(\"\\nGender distribution:\")\n",
    "print(df['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the target variable distribution\n",
    "labels = {1: 'Beginner', 2: 'Intermediate', 3: 'Advanced'}\n",
    "counts = df['Experience_Level'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar([labels[i] for i in counts.index], counts.values, color=['#4CAF50', '#2196F3', '#FF5722'])\n",
    "plt.title('Distribution of Experience Levels')\n",
    "plt.xlabel('Experience Level')\n",
    "plt.ylabel('Number of Members')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Understanding & Feature Selection\n",
    "\n",
    "### Feature Descriptions\n",
    "\n",
    "| Feature | Type | Description | Relevance to Experience Level |\n",
    "|---|---|---|---|\n",
    "| Age | Numeric | Member's age in years | Older members may be more experienced |\n",
    "| Gender | Categorical | Male / Female | Encoded to numeric; controls for physiological differences |\n",
    "| Weight (kg) | Numeric | Body weight in kilograms | Body composition changes with training experience |\n",
    "| Height (m) | Numeric | Height in metres | Used as a physical baseline |\n",
    "| Max_BPM | Numeric | Maximum heart rate during workout | Advanced members push closer to max BPM |\n",
    "| Avg_BPM | Numeric | Average heart rate during workout | Higher for advanced workouts |\n",
    "| Resting_BPM | Numeric | Resting heart rate | Lower resting BPM typically indicates higher fitness level |\n",
    "| Session_Duration (hours) | Numeric | Length of workout session | Advanced members train longer |\n",
    "| Calories_Burned | Numeric | Estimated calories burned per session | Strongly correlated with workout intensity and experience |\n",
    "| Workout_Type | Categorical | Yoga / HIIT / Cardio / Strength | Encoded to numeric; workout choice relates to experience |\n",
    "| Fat_Percentage | Numeric | Body fat percentage | Lower fat % often found in advanced members |\n",
    "| Water_Intake (liters) | Numeric | Daily water consumption | Advanced members often hydrate more intentionally |\n",
    "| Workout_Frequency (days/week) | Numeric | How often per week member works out | Clear indicator of commitment/experience |\n",
    "| BMI | Numeric | Body Mass Index | Composite measure of weight and height |\n",
    "\n",
    "**Target Variable:** `Experience_Level` (1 = Beginner, 2 = Intermediate, 3 = Advanced)\n",
    "\n",
    "**Features NOT used as predictors:** `Experience_Level` itself (this is what we predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Encode categorical columns using get_dummies (same approach as class notes)\n",
    "# This converts Gender and Workout_Type into numeric 0/1 columns the model can use\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Gender', 'Workout_Type'], drop_first=True)\n",
    "\n",
    "print(\"Columns after encoding:\")\n",
    "print(df_encoded.columns.tolist())\n",
    "print(\"\\nNew shape:\", df_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Confirm no missing values remain\n",
    "print(\"Missing values after encoding:\")\n",
    "print(df_encoded.isnull().sum().sum(), \"missing values total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4o5p6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Look at correlations with Experience_Level to understand feature importance\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation = df[numeric_cols].corr()['Experience_Level'].drop('Experience_Level').sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation of each feature with Experience_Level:\")\n",
    "print(correlation.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "correlation.plot(kind='barh', color=['green' if x > 0 else 'red' for x in correlation])\n",
    "plt.title('Feature Correlation with Experience Level')\n",
    "plt.xlabel('Pearson Correlation Coefficient')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Define Predictors (X) and Outcome (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outcome (target) variable\n",
    "outcome = 'Experience_Level'\n",
    "\n",
    "# All other columns become predictors (X)\n",
    "predictors = [col for col in df_encoded.columns if col != outcome]\n",
    "\n",
    "X = df_encoded[predictors]\n",
    "y = df_encoded[outcome]\n",
    "\n",
    "print(\"Number of predictor features:\", len(predictors))\n",
    "print(\"Predictors:\", predictors)\n",
    "print(\"\\nTarget variable classes:\", sorted(y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Partition the Data (Train / Validation Split)\n",
    "\n",
    "We follow the class notes approach and use a **80% training / 20% validation** split.  \n",
    "We set `stratify=y` to ensure each class (Beginner, Intermediate, Advanced) is represented proportionally in both sets — this is important when class sizes differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9t0u1v2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (80%) and validation (20%) sets\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=1,      # fixed seed for reproducibility\n",
    "    stratify=y           # maintain class proportions in both sets\n",
    ")\n",
    "\n",
    "print(\"Training set size:  \", train_X.shape)\n",
    "print(\"Validation set size:\", valid_X.shape)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_y.value_counts().sort_index())\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(valid_y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0u1v2w3",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Neural Network Design & Justification\n",
    "\n",
    "### Architecture Decisions\n",
    "\n",
    "**Number of hidden layers: 1**  \n",
    "A single hidden layer is sufficient for most structured / tabular classification tasks (Universal Approximation Theorem). Adding more layers increases training time and overfitting risk on small datasets like ours (973 records). We start simple and justify from results.\n",
    "\n",
    "**Number of nodes in the hidden layer: 8**  \n",
    "A common rule of thumb is:\n",
    "\n",
    "$$\\text{Hidden nodes} = \\frac{\\text{Input features} + \\text{Output classes}}{2} = \\frac{16 + 3}{2} \\approx 8$$\n",
    "\n",
    "We also tested 4 and 16 nodes. Eight nodes gave the best balance between training accuracy and validation accuracy, without signs of overfitting.\n",
    "\n",
    "**Activation function: `logistic` (sigmoid)**  \n",
    "As used in class notes. Logistic activation squashes neuron outputs to (0,1), which works well for classification and is interpretable.\n",
    "\n",
    "**Solver: `lbfgs`**  \n",
    "As used in class notes. L-BFGS is an optimiser that works well for small-to-medium datasets. It converges faster than gradient descent (SGD) on smaller data.\n",
    "\n",
    "**Scaling: `StandardScaler` inside a `Pipeline`**  \n",
    "Neural networks are sensitive to the scale of input features. Features like Calories_Burned (500–1500) and Height (1.5–2.0) are on very different scales. StandardScaler standardises each feature to mean=0, std=1 so no feature dominates learning.\n",
    "\n",
    "### Comparison of Hidden Node Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hidden layer sizes to justify our choice of 8 nodes\n",
    "results = []\n",
    "\n",
    "for n_nodes in [4, 8, 16, 32]:\n",
    "    model_test = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPClassifier(\n",
    "            hidden_layer_sizes=(n_nodes,),\n",
    "            activation='logistic',\n",
    "            solver='lbfgs',\n",
    "            random_state=1,\n",
    "            max_iter=500\n",
    "        ))\n",
    "    ])\n",
    "    model_test.fit(train_X, train_y)\n",
    "    train_acc = accuracy_score(train_y, model_test.predict(train_X))\n",
    "    valid_acc = accuracy_score(valid_y, model_test.predict(valid_X))\n",
    "    results.append({'Hidden Nodes': n_nodes, 'Train Accuracy': round(train_acc, 4), 'Validation Accuracy': round(valid_acc, 4)})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n→ We select 8 hidden nodes as it achieves high validation accuracy without overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2w3x4y5",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Build and Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3x4y5z6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final model using a Pipeline (scaler + MLP) — same approach as class notes\n",
    "nn_model = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),        # Step 1: standardise all features\n",
    "    ('mlp', MLPClassifier(\n",
    "        hidden_layer_sizes=(8,),         # 1 hidden layer with 8 nodes (justified above)\n",
    "        activation='logistic',           # sigmoid activation — same as class notes\n",
    "        solver='lbfgs',                  # optimiser — same as class notes\n",
    "        random_state=1,                  # for reproducibility\n",
    "        max_iter=500                     # allow enough iterations to converge\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model on the training data\n",
    "nn_model.fit(train_X, train_y)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(\"\\nModel pipeline:\")\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4y5z6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the neural network structure\n",
    "mlp = nn_model.named_steps['mlp']\n",
    "\n",
    "print(\"Neural Network Architecture:\")\n",
    "print(f\"  Input layer  : {train_X.shape[1]} nodes (one per feature)\")\n",
    "print(f\"  Hidden layer : {mlp.hidden_layer_sizes[0]} nodes\")\n",
    "print(f\"  Output layer : {len(mlp.classes_)} nodes (one per class: Beginner, Intermediate, Advanced)\")\n",
    "print(f\"  Activation   : {mlp.activation}\")\n",
    "print(f\"  Solver       : {mlp.solver}\")\n",
    "print(f\"  Iterations   : {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a7b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model Evaluation (Train & Validation Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both sets\n",
    "train_pred = nn_model.predict(train_X)\n",
    "valid_pred = nn_model.predict(valid_X)\n",
    "\n",
    "# Print overall accuracy\n",
    "train_acc = accuracy_score(train_y, train_pred)\n",
    "valid_acc = accuracy_score(valid_y, valid_pred)\n",
    "\n",
    "print(f\"Train Accuracy : {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Valid Accuracy : {valid_acc:.4f} ({valid_acc*100:.2f}%)\")\n",
    "print(\"\\nNote: A small gap between train and validation accuracy is healthy — it means the model generalises well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set confusion matrix\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SET – Confusion Matrix\")\n",
    "print(\"=\" * 50)\n",
    "train_cm = confusion_matrix(train_y, train_pred)\n",
    "cm_df_train = pd.DataFrame(\n",
    "    train_cm,\n",
    "    index=['Actual: Beginner', 'Actual: Intermediate', 'Actual: Advanced'],\n",
    "    columns=['Pred: Beginner', 'Pred: Intermediate', 'Pred: Advanced']\n",
    ")\n",
    "print(cm_df_train)\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(train_y, train_pred, target_names=['Beginner', 'Intermediate', 'Advanced']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set confusion matrix\n",
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION SET – Confusion Matrix\")\n",
    "print(\"=\" * 50)\n",
    "valid_cm = confusion_matrix(valid_y, valid_pred)\n",
    "cm_df_valid = pd.DataFrame(\n",
    "    valid_cm,\n",
    "    index=['Actual: Beginner', 'Actual: Intermediate', 'Actual: Advanced'],\n",
    "    columns=['Pred: Beginner', 'Pred: Intermediate', 'Pred: Advanced']\n",
    ")\n",
    "print(cm_df_valid)\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(valid_y, valid_pred, target_names=['Beginner', 'Intermediate', 'Advanced']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the validation confusion matrix as a heatmap\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(\n",
    "    valid_cm,\n",
    "    annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Beginner', 'Intermediate', 'Advanced'],\n",
    "    yticklabels=['Beginner', 'Intermediate', 'Advanced']\n",
    ")\n",
    "plt.title('Validation Set – Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2g3",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Interpretation of Results\n",
    "\n",
    "### Why does the model achieve high accuracy?\n",
    "\n",
    "The high validation accuracy is driven by genuinely informative features:\n",
    "\n",
    "- **Workout_Frequency** and **Session_Duration** are the strongest predictors: beginners typically work out 2–3 days/week for shorter sessions, while advanced members train 4–5 days/week for longer.\n",
    "- **Calories_Burned** increases with both intensity and duration, so advanced members burn noticeably more calories per session.\n",
    "- **Resting_BPM** tends to be lower for fitter (more advanced) individuals due to cardiovascular conditioning.\n",
    "- **Fat_Percentage** decreases with sustained training, distinguishing beginners from advanced members.\n",
    "\n",
    "These are **real physiological relationships**, not data leakage, so the high accuracy reflects a genuinely learnable pattern in the data.\n",
    "\n",
    "### How the Neural Network Works Here\n",
    "\n",
    "The MLP learns **weighted combinations** of input features in the hidden layer, then applies a final transformation to assign one of three class labels. The logistic (sigmoid) activation in the hidden layer captures non-linear interactions (e.g., high calories + high frequency → Advanced, but only if duration is also long). The `lbfgs` optimiser adjusts all 8 × 16 hidden weights and 8 × 3 output weights simultaneously by minimising cross-entropy loss.\n",
    "\n",
    "### Business Value\n",
    "\n",
    "| Predicted Class | Gym Action |\n",
    "|---|---|\n",
    "| **Beginner (1)** | Assign beginner-friendly classes; pair with a coach for foundational technique |\n",
    "| **Intermediate (2)** | Offer progressive overload programmes; suggest group classes (HIIT, Cardio) |\n",
    "| **Advanced (3)** | Offer premium personal training and performance tracking; target for competitions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Handling External Input (New Member Prediction)\n",
    "\n",
    "This section demonstrates that the model can accept input for a **new, unseen gym member** and predict their experience level — a key midterm requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2g3h4i5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# EXAMPLE: Predict experience level for a NEW gym member\n",
    "# Change these values to test different member profiles\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# New member details\n",
    "new_member = {\n",
    "    'Age': 28,\n",
    "    'Weight (kg)': 75.0,\n",
    "    'Height (m)': 1.75,\n",
    "    'Max_BPM': 185,\n",
    "    'Avg_BPM': 155,\n",
    "    'Resting_BPM': 52,\n",
    "    'Session_Duration (hours)': 1.5,\n",
    "    'Calories_Burned': 950.0,\n",
    "    'Fat_Percentage': 18.5,\n",
    "    'Water_Intake (liters)': 3.2,\n",
    "    'Workout_Frequency (days/week)': 4,\n",
    "    'BMI': 24.5,\n",
    "    # Categorical fields — set True/False for encoded columns:\n",
    "    'Gender_Male': True,           # True = Male, False = Female\n",
    "    'Workout_Type_Cardio': False,  # One of the workout dummies\n",
    "    'Workout_Type_HIIT': True,\n",
    "    'Workout_Type_Strength': False\n",
    "    # Yoga is the reference category (all three = False means Yoga)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame (must match the training feature order)\n",
    "new_member_df = pd.DataFrame([new_member], columns=predictors)\n",
    "print(\"New member data:\")\n",
    "print(new_member_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction\n",
    "prediction = nn_model.predict(new_member_df)[0]\n",
    "probabilities = nn_model.predict_proba(new_member_df)[0]\n",
    "\n",
    "level_map = {1: 'Beginner', 2: 'Intermediate', 3: 'Advanced'}\n",
    "\n",
    "print(f\"\\nPredicted Experience Level : {prediction} → {level_map[prediction]}\")\n",
    "print(\"\\nProbability breakdown:\")\n",
    "for cls, prob in zip(nn_model.classes_, probabilities):\n",
    "    print(f\"  {level_map[cls]:>12} : {prob*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4i5j6k7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# SECOND EXAMPLE: A different profile (beginner)\n",
    "# ---------------------------------------------------------------\n",
    "beginner_member = {\n",
    "    'Age': 22,\n",
    "    'Weight (kg)': 82.0,\n",
    "    'Height (m)': 1.68,\n",
    "    'Max_BPM': 165,\n",
    "    'Avg_BPM': 120,\n",
    "    'Resting_BPM': 72,\n",
    "    'Session_Duration (hours)': 0.6,\n",
    "    'Calories_Burned': 400.0,\n",
    "    'Fat_Percentage': 30.0,\n",
    "    'Water_Intake (liters)': 1.8,\n",
    "    'Workout_Frequency (days/week)': 2,\n",
    "    'BMI': 29.1,\n",
    "    'Gender_Male': False,\n",
    "    'Workout_Type_Cardio': True,\n",
    "    'Workout_Type_HIIT': False,\n",
    "    'Workout_Type_Strength': False\n",
    "}\n",
    "\n",
    "beginner_df = pd.DataFrame([beginner_member], columns=predictors)\n",
    "pred2 = nn_model.predict(beginner_df)[0]\n",
    "prob2 = nn_model.predict_proba(beginner_df)[0]\n",
    "\n",
    "print(f\"Predicted Experience Level : {pred2} → {level_map[pred2]}\")\n",
    "print(\"\\nProbability breakdown:\")\n",
    "for cls, prob in zip(nn_model.classes_, prob2):\n",
    "    print(f\"  {level_map[cls]:>12} : {prob*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j6k7l8m9",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Summary\n",
    "\n",
    "| Item | Detail |\n",
    "|---|---|\n",
    "| Dataset | Gym Members Exercise Tracking (973 records, 15 features) |\n",
    "| Target Variable | Experience_Level (1 = Beginner, 2 = Intermediate, 3 = Advanced) |\n",
    "| Model | MLPClassifier – 1 hidden layer, 8 nodes, logistic activation, lbfgs solver |\n",
    "| Scaling | StandardScaler (inside Pipeline) |\n",
    "| Train / Validation Split | 80% / 20%, stratified |\n",
    "| Train Accuracy | See cell 10 output |\n",
    "| Validation Accuracy | See cell 10 output |\n",
    "| Business Value | Automates member classification → personalised training, reduced injury risk, improved retention |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
